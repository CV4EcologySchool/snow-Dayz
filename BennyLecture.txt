1. Workspace Set-up 

    Avoid spaghetti code! You want to *organize* code folders 

    ##  Objectives 
    * separate code from experiments from data
    * explain how to install & run project (also for yourself)
    * make project reproducible
    * define polices (access, sharing, etc.)

    Example set-up:
    * .git 
    * readme.md
        The welcome screen. Markdown formatted.
        Overview, Background, installation, usage instructions, citation information
        Don't make the code public until you have published 
    * requirements.txt
        list packages of python and makes installation easier
        "auxiliary" packages, less important packages,version can be a >= or  = 
        Use == if the package version really matters
            torch == 1.9.0+cu1111
        Benny does it manually 
        Automatically: pip freeze > requirements.txt
        conda env export > environment.yaml 
            can then re-create the packages automatically conda env create -f environment.yaml 
        Run this on the command on the workspace 
    * license
    * .gitignore
        list files and folders NOT to be uploaded to your GitHub 
        model save states (cnn_states)
        don't forget that "." at the beginning means that it is a 
    * camera_trapinator
        this is where we put all the files!

2. Publish to GitHub, GitLab, etc.
    -- be extra effort at the beginning but in the end it will be worth it!
    -- Have a folder with data that is OUTSIDE github project

3. Object-Oriented Programming (OOP)
    We will use this a little bit in programming 
    So far, we have occassional functions, and it is a script 

    OOP: 
        object class: blueprint, properties, functionality // species
        object instance: Actual example for object class with property values species 
            // species individuals

        Animal (object class)
        - properties: species , age, sex
        - functions: make_sound()
        Cat #1 (object instance)
            - properties: species cat / 12/ make
        Cat #2 (object instance): properties = dog/age 3/ female 

    For us: model = class  
        instance = model #1 with certain paramters, and model #2 with another set of parameters
    
    OR class called dataset
        split it one way 
    
    To define a class:  First define class, 
        a. within it define properites (instances
        b. define other functions 
        Common classes in deep learning: datasets, models
            models all functions called : fit, predict

4. PyTorch and Deep Learning : Overview
    * Data loading and data augmentation 
        input image and ground truth 
    * Model -- convolution, max pool, ReLU ..., softmax 
        Forward pass (push image through the model) = prediction 
    * Compare the two, with the loss function (or criterion function). Examples:
        Regression -Least squares
        Classification - cross entropy (segmentation)
    * Now we do a backward pass (backpropagation)
        Mathematically, we take the first derivative of the loss function, and then use the chain rule 
        We get gradient values, and tell you how to minimize each parameter 
        With each image it learns (it is important to randomly expose the model to all the datapoints)
        There is a way to "gradually expose the image to harder and harder images"
            First only expose the images to hard images, or the images of focus 

5. Deep learning in Practice
    * prototyping the model 
    * Now we do large scale model training 
    * Commercial implementations that run on cell phone

6. Pytorch -- what and why? 
    * pytorch has numpy and scipy functionality 
    * has GPUs compatibility 
    * models get so big 
     
    * Pytorch specialicities 
    - can create a tensor that sits in memory 
    - can move it to .cuda() # move tensor from system memory to GPU 
    - in conventional systems it will default to RAM, so we need to transfer to GPU because it is 
        better at model operations

7. Deep learning code setup 
    - dataset.py: pytorch wrapper to load your dataset 
    - model.py: pytorch implementation of yoru deep learning mode, 
    - train.py script to train your model -- feels more script -like!
    - test.py script to test your model & and just predict if you want

dataset.python 
    functions:
    - init function 
    - __len__ : how big it is 
    - __getitem__ : slice/pluck an item and open 
        load it into memory, here is where we will access the GPUs! 
        ***can import a base class (instead of self, call the class you want to inherit)

Example scripts
    dataloader.py 
        * class dataset

    model.py 
        * class model
        1. Define layers as functions
        2. We define forward pass
        3. 

    train.py   
        dataloader = DataLoader(dataset, num_workers) ## can tune for maximum speed
        model = UNet()
        optimizer = SGD(model.parameters()), learning_rate) ## how and when to apply the gradients 
            ### 
        for loop 
            * iteriate through data, make sure to put on GPU using .cuda()
            * criterion(pred,labels) --> loss 
            * optimizer.zero_grad() --> you don't want the gradients from the previous iteration, 
            so you set them to zero 
            * loss.backward() ## calc gradients (backward pass)
            * optimizer.step() ## update model parameters wrt gradients

Pitfalls 
- pytorch needs to be the same number type, size and on the same GPU (.cuda)
- numpy doesn't understand GPU, so first to needs 
- never call backward twice! 
- stop autograd // 
    - pred1 = model1(data)
    - pred2 = model2(data.detach()) ## removes the gradient descent and back propogation and 
    makes the mode a lot faster
    




