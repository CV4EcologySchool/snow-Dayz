

What is your baseline?

binary classification? no sequences

What ideas do you want to try? 
- adding class weighting
- add in additional data (additonal weather images from internet (from cities))
- 

How can we demonstrate progress? 
There are trade-offs between your models

A "fair" comparison balances across all the metrics
- Preprocessing, Architecture, and loss --> performacne should be weighed across all three methods 

What makes a comparison fiar/ 
WHat makes a compaison unfiar? 

A fair comparison should be comparing performance across all three categories (preprocessing architecture and loss) 
and weighing the tradeoffs inherent in all three

What is fair ?
Changing one thing and considering performance? 

Do we want to use the same? 
- preprocessing?
- Architecture? 
- Hyperparameter values? (learning rate, batch size, etc)
- Hyperparameters should be 
- they probably spent a lot more time on the rpoposed method than the baseline


What would be an example of an unfair comparison between this idea and your baseline? Why would it be unfair? 
- changing architecture? 
- preprocessing? (Using the full image?)
- throwing in more data from internet weather && doing something else? --> curriculum learning ???
- having two different sampling rates for audio rates 



Can you think of of any way  to improve the performance Method B? 
- is Method A > Method B even if I give Method B unfair advantages? Stronger evidence that method A is better! 
Have I invested at least as much effort in tuning Method B as i have in tuning Method A?

>>> ask Sara / Eli what would be a fair comparison in my study?
-lowering epochs 

Ablation studies
- Adding X and Y helps more than adding either alone
- Baseline 
- Baseline + X
- Baseline + Y 
- Baseline + X + Y  

Ask Eli or Sara: 
+++++++++++++++++++++++++++++++++++++++++
X : sequences cleaning          ~
Y: more internet data 
Z: curriculum learning 
ZZ: multiple classes  ~
ZZZ: play with different weights  
++++++++++++++++++++++++++++++++++++++++

Worth it to add Y? 
- complexity can add significantly to the model 

Ablation examples!!
- there are a lot of examples for augmentation -- industry property 
- usually there are one or axis of transformations


Can that idea be broken up into components?? Write it into your dataloader
- put it into your dataloader -> have on and off switches?? for extra data or not
- curriculum learning??? not really sure how to build that in 

What kind of ablation study would you rub to analyze the contribution of each component? 

What is commonly ablated? 
- terms in loss functions
- preprocessing steps (inc augmentations)
- training data sources and quantities
- training duration (1000 epochs vs. 100 epochs etc) --> this comes for free with checkpoints 

Stable Evaluation: 
- Performance comparison often involves randomness 
    - A is always better than B, but sometimes 

If you have unstable evaluation 
- run model multiple times, compare mean and std dev


An ablation study is a set of fair comparisons which show the benefit of each component of method 

What is the one thing shoudl we do for this week? 
-- do order of magnitudes for learning rate 1e-2 1e-4 
-- usually the bigger the batch size, the bigger your learning rate needs to be 









