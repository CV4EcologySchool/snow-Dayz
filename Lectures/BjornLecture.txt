

Acquiring large labeled datasets is costly 

Issues: 

Generalization with Neural Networks can be brittle

Trained on Maxar satellite imagery 

How to quantify out-of-distribution?
    - Human evaluation 
    - Distance in Euclidean space 
        - Regression 
        - 
    - Distance in feature space

Augmentation aims to convert extrapolation to interpolation
    - data augmentation incrases the size of the train ndata aiming for the test to becoem a subset of the train data. 
    - you have to use your domain knowledge to know what axis you can change 
        - exposure, horizontal flip, gaussian blur, rotational 

Common axes of variation 
- Differning environmental conditions (day, night, fog, snow, rain, haze, lighting shadows)
- RS: atmospheric noise, sun inclination
- A: background noises

Differing sensors
- Im: resolution, aperture, 

Augmentations incoroporate domain knowledge
Goal: introduce domain knowledge and synthetic variation that does not change the label (invariance) or changes the label in a known way (equivariance)

We would still expect Snoop Dog to be snoop dog no matter the hat he is wearing!

How do we implement? There are libraries available! 
- Flipping, 
- Random Crop and Resize 

For weather: 
Do's: 
- Crop a tiny square
- increase brightness
- Flipping 
- day vs night 
- convert everything to a grey scale
Don'ts: 
- introducing weather (overlay clouds or rain) --> use random weather! built-in in python
- noise / guassian, drop out noise (tiny black values)

Adversarial noise - if you change a pixel it can go from a beaver to a panda and make our model 
-

Mixup//CutMix
    take two images 


Define an augmentation pipeline 
- 