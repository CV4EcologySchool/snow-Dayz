{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import glob\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CTDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import create_dataloader, load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CTDataset \n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import glob\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from train import create_dataloader, load_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datadrive/vmData/weather/trainLabels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000004?line=2'>3</a>\u001b[0m cfg \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39msafe_load(\u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m/Users/cmbreen/code/snow-Dayz/configs/exp_resnet50_2classes.yaml\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000004?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m/Users/cmbreen/Downloads/experiment_name/model_states/2.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000004?line=4'>5</a>\u001b[0m dataLoader\u001b[39m=\u001b[39m create_dataloader(cfg, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, labels \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrainLabels.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, folder \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/code/snow-Dayz/classifier/train.py:37\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(cfg, split, labels, folder)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m    Loads a dataset according to the provided split and wraps it in a\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m    PyTorch DataLoader object.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m#labels = os.path.join(self.data_root, labels) ## if the full path above doesn't work\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m#dataset_instance = CTDataset(cfg, split)        # create an object instance of our CTDataset class\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m dataset_instance \u001b[39m=\u001b[39m CTDataset(labels, cfg, folder, split)\n\u001b[1;32m     39\u001b[0m dataLoader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     40\u001b[0m         dataset\u001b[39m=\u001b[39mdataset_instance,\n\u001b[1;32m     41\u001b[0m         batch_size\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     42\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m         num_workers\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m dataLoader\n",
      "File \u001b[0;32m~/code/snow-Dayz/classifier/dataset.py:66\u001b[0m, in \u001b[0;36mCTDataset.__init__\u001b[0;34m(self, labels, cfg, folder, split, sequenceType)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# load annotation file\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannoPath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_root, labels) \u001b[39m############# should i set this as an input?? \u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m meta \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannoPath)\n\u001b[1;32m     67\u001b[0m meta \u001b[39m=\u001b[39m meta[meta[\u001b[39m'\u001b[39m\u001b[39mWeather\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFog\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m### could also drop 'other'\u001b[39;00m\n\u001b[1;32m     68\u001b[0m meta \u001b[39m=\u001b[39m meta[meta[\u001b[39m'\u001b[39m\u001b[39mWeather\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mOther\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cv4e/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cv4e/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cv4e/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cv4e/lib/python3.8/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cv4e/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cv4e/lib/python3.8/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datadrive/vmData/weather/trainLabels.csv'"
     ]
    }
   ],
   "source": [
    "## linux path: /home/azureuser/snow-Dayz/configs/exp_resnet50_2classes.yaml\n",
    "### local path \n",
    "cfg = yaml.safe_load(open('//home/azureuser/snow-Dayz/configs/exp_resnet50_2classes.yaml'))\n",
    "model = torch.load('/home/azureuser/snow-Dayz/experiments/experiment_name/model_states/5.pt')\n",
    "#model1 = torch.load_state_dict('/home/azureuser/snow-Dayz/experiments/experiment_name/model_states/5.pt')\n",
    "dataLoader= create_dataloader(cfg, split='train', labels = 'trainLabels.csv', folder = 'train')\n",
    "exp_name = 'experiment_name'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = CustomResNet50(cfg['num_classes'])         # create an object instance of our CustomResNet18 class\n",
    "\n",
    "    # load latest model state\n",
    "model_states = glob(exp_name+'/model_states/*.pt')\n",
    "\n",
    "if len(model_states) > 0:\n",
    "        # at least one save state found; get latest\n",
    "    model_epochs = [int(m.replace({exp_name}+'/model_states/','').replace('.pt','')) for m in model_states]\n",
    "    if epoch:\n",
    "        start_epoch = epoch\n",
    "    else:\n",
    "        start_epoch = max(model_epochs)\n",
    "\n",
    "        # load state dict and apply weights to model\n",
    "    print(f'Evaluating from epoch {start_epoch}')\n",
    "    state = torch.load(open(f'{exp_name}/model_states/{start_epoch}.pt', 'rb'), map_location='cpu')\n",
    "    model_instance.load_state_dict(state['model'])\n",
    "\n",
    "        #import IPython\n",
    "        #IPython.embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(cfg, dataLoader, model):\n",
    "    with torch.no_grad(): # no gradients needed for prediction\n",
    "        predictions = []\n",
    "        predict_labels = [] \n",
    "        labels = []\n",
    "        confidences = []\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        ##### may need to adjust this in the dataloader for the sequence:\n",
    "        for idx, (data, label) in enumerate(dataLoader): \n",
    "            prediction = model(data) ## the full probabilty\n",
    "            predict_label = torch.argmax(prediction, dim=1) ## the label\n",
    "            confidence = torch.nn.Softmax(prediction)\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        predict_labels.append(int(predict_label))\n",
    "        labels.append(int(label))\n",
    "        confidences.append(int(confidence))\n",
    "\n",
    "    return predictions, predict_labels, labels, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad(): # no gradients needed for prediction\n",
    "    predictions = []\n",
    "    predict_labels = [] \n",
    "    labels = []\n",
    "    confidences = []\n",
    "        ##### may need to adjust this in the dataloader for the sequence:\n",
    "    for idx, (data, label) in enumerate(dataLoader): \n",
    "        prediction = model(data) ## the full probabilty\n",
    "        predict_label = torch.argmax(prediction, dim=1) ## the label\n",
    "        confidence = torch.nn.Softmax(prediction)\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    predict_labels.append(int(predict_label))\n",
    "    labels.append(int(label))\n",
    "    confidences.append(int(confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataLoader, model):\n",
    "  with torch.no_grad():   # no gradients needed for prediction\n",
    "    predictions = []\n",
    "    predict_labels = []\n",
    "    labels = []\n",
    "    data = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx, (data, label) in enumerate(dataLoader): \n",
    "      prediction = model(data) \n",
    "      predict_label = torch.argmax(prediction, dim=1) \n",
    "\n",
    "      predictions.append(prediction)\n",
    "      predict_labels.append(int(predict_label))\n",
    "      labels.append(int(label))\n",
    "\n",
    "    return data, predictions, predict_labels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuzzy_accuracy(y_true, y_pred):\n",
    "    # OA: number of correct predictions divided by batch size (i.e., average/mean)\n",
    "    facc = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if pred in range(true - 1, true + 1, 1):\n",
    "            facc += 1\n",
    "    \n",
    "    facc /= len(y_true)\n",
    "\n",
    "    return facc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(y_true, y_pred, exp_name, epoch, split):\n",
    "    # make figures folder if not there\n",
    "    os.makedirs(exp_name+'/figs', exist_ok=True)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot()\n",
    "    plt.savefig(exp_name+'/figs/confusion_matrix_epoch'+str(epoch)+'_'+str(split)+'.png', facecolor=\"white\")\n",
    "    \n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model directory\n",
    "#outdir = exp_name\n",
    "\n",
    "    # get config from model directory\n",
    "config = glob(exp_name+'*.yaml')[0]\n",
    "\n",
    "    # load config\n",
    "print(f'Using config \"{config}\" and using \"{args.split}\" set')\n",
    "#cfg = yaml.safe_load(open(config, 'r'))\n",
    "\n",
    "    # setup dataloader\n",
    "dl_val = create_dataloader(cfg, split=args.split, batch=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# load model and predict from model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000010?line=1'>2</a>\u001b[0m model, epoch \u001b[39m=\u001b[39m load_model(cfg, outdir, args\u001b[39m.\u001b[39mepoch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000010?line=2'>3</a>\u001b[0m data, predictions, predict_labels, labels \u001b[39m=\u001b[39m predict(dl_val, model)   \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cmbreen/code/snow-Dayz/classifier/test_evaluate.ipynb#ch0000010?line=4'>5</a>\u001b[0m \u001b[39m# get accuracy score\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outdir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    # load model and predict from model\n",
    "#model, epoch = load_model(cfg, exp_name, args.epoch)\n",
    "data, predictions, predict_labels, labels = predict(dl_val, model)   \n",
    "    \n",
    "    # get accuracy score\n",
    "acc = accuracy_score(labels, predict_labels)\n",
    "print(\"Accuracy of model is {:0.2f}\".format(acc))\n",
    "    \n",
    "    # get fuzzy accuracy\n",
    "facc = get_fuzzy_accuracy(labels, predict_labels)\n",
    "print(\"Accuracy within 1 class is {:0.2f}\".format(facc))\n",
    "\n",
    "    # confusion matrix\n",
    "cm = save_confusion_matrix(labels, predict_labels, exp_name, epoch, args.split)\n",
    "\n",
    "    # save list of predictions with filename\n",
    "df = pd.DataFrame({'filename': data,\n",
    "                    #'predictions': predictions,\n",
    "                    'predict_label': predict_labels,\n",
    "                    'real_label': labels}\n",
    "    )\n",
    "df.to_csv(outdir+'/results_epoch'+str(epoch)+'_'+str(args.split)+'.csv', index = False)\n",
    "\n",
    "    # precision recall curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a61ff342c82320e16bfc3ec00598c30076a67172fe0ba42bf7d90ff352a912bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
